1
00:00:02,340 --> 00:00:04,340
字幕生成：慎独     字幕校对：lim

2
00:00:04,925 --> 00:00:06,584
大家好,我是ZOMI

3
00:00:06,609 --> 00:00:08,975
今天我们来到AI编译器里面的

4
00:00:08,975 --> 00:00:11,581
AI的计算体系的第一个内容

5
00:00:11,581 --> 00:00:14,909
就去讲讲深度学习的计算模式。

6
00:00:14,909 --> 00:00:17,775
那所谓的深度学习的计算模式呢

7
00:00:17,775 --> 00:00:19,503
就在我们的第一节内容

8
00:00:19,503 --> 00:00:22,767
它主要是指我们现在要讲的几个内容。

9
00:00:22,767 --> 00:00:23,471
第一个呢

10
00:00:23,471 --> 00:00:26,735
AI的发展还有它的一个基本的范式

11
00:00:26,735 --> 00:00:29,039
然后呢,我们去看看深度学习

12
00:00:29,039 --> 00:00:32,623
或者AI里面的几个经典的网络模型结构

13
00:00:32,623 --> 00:00:34,031
那这里面其实很简单

14
00:00:34,031 --> 00:00:36,271
就是ResNet, EfficientNet, MobileNet

15
00:00:36,271 --> 00:00:40,275
接着去看看模型的量化和剪枝

16
00:00:40,275 --> 00:00:42,325
相关的网络压缩的模块

17
00:00:42,325 --> 00:00:45,653
下面我们会去看看轻量化的网络模型

18
00:00:45,653 --> 00:00:46,421
MobileNet

19
00:00:46,421 --> 00:00:49,173
那些能够在手机端侧部署的模型

20
00:00:49,173 --> 00:00:52,206
最后我们看看大模型 还有分布式并行

21
00:00:52,654 --> 00:00:55,000
这里面内容稍微有点多

22
00:00:55,000 --> 00:00:57,975
所以我会分开两个视频给大家去汇报

23
00:00:57,975 --> 00:01:00,990
那第一个呢，我们会去讲讲前面三个内容

24
00:01:00,990 --> 00:01:03,612
第二个视频呢, 我们讲讲后面两个内容

25
00:01:03,612 --> 00:01:06,940
注意，我们这里面非常关心的是计算模式

26
00:01:07,315 --> 00:01:08,790
通过了解我们AI的发展

27
00:01:08,790 --> 00:01:10,259
了解我们神经网络的发展

28
00:01:10,259 --> 00:01:12,434
了解我们人工智能的发展

29
00:01:12,434 --> 00:01:16,083
去看看对于我们硬件的计算模式的改变

30
00:01:16,083 --> 00:01:17,235
或者我们的牵引

31
00:01:17,235 --> 00:01:19,027
或者一些suggestion也好

32
00:01:19,923 --> 00:01:22,227
我们进入正式的内容里面

33
00:01:22,227 --> 00:01:23,251
首先第一个就是

34
00:01:23,251 --> 00:01:26,323
AI的发展和AI的范式

35
00:01:26,323 --> 00:01:29,046
这个图就是AI总体的发展路线

36
00:01:29,046 --> 00:01:31,222
从1950年代那时候开始

37
00:01:31,222 --> 00:01:32,981
已经提出了人工智能

38
00:01:32,981 --> 00:01:34,773
接着在八九十年代的时候

39
00:01:34,773 --> 00:01:37,077
机器学习是非常非常的火

40
00:01:37,077 --> 00:01:40,868
而深度学习真正迎来爆发是在2010年之后

41
00:01:40,868 --> 00:01:42,596
其实我们迎来更重要

42
00:01:42,596 --> 00:01:44,836
更加outstanding的一种技术

43
00:01:44,836 --> 00:01:46,116
叫做Foundation Model

44
00:01:46,116 --> 00:01:47,972
那为啥我最近更新的特别慢呢

45
00:01:47,972 --> 00:01:50,532
是因为确实在公司非常注重大模型

46
00:01:50,532 --> 00:01:52,708
而ChatGPT大模型这个技术

47
00:01:52,708 --> 00:01:54,901
就是我在负责的一个主要的项目

48
00:01:54,901 --> 00:01:57,312
所以呢就会经常脱更

49
00:01:57,312 --> 00:01:59,312
因为确实工作压力太大了

50
00:02:00,457 --> 00:02:02,057
下面呢, 回到正题

51
00:02:02,057 --> 00:02:04,057
我们看看AI的三大范式

52
00:02:04,057 --> 00:02:06,057
也就是机器学习的三大范式

53
00:02:06,057 --> 00:02:10,409
主要有强化学习, 监督学习, 还有无监督学习

54
00:02:10,409 --> 00:02:12,073
那不管AI的三种方式呢

55
00:02:12,073 --> 00:02:13,033
其实我们现在

56
00:02:13,033 --> 00:02:16,361
都可以用深度学习计算模式去使用

57
00:02:16,361 --> 00:02:17,257
那我们现在看看

58
00:02:17,257 --> 00:02:19,305
深度学习里面跟这三种模式结合

59
00:02:19,305 --> 00:02:21,353
有哪些不一样或者有哪些区别

60
00:02:21,353 --> 00:02:22,953
或者有哪些结合点

61
00:02:22,953 --> 00:02:24,361
那下面我们可以看到呢

62
00:02:24,361 --> 00:02:26,361
像这种, 假设我们绿色的框框

63
00:02:26,361 --> 00:02:27,960
就是一个深度学习的模型

64
00:02:27,960 --> 00:02:30,125
在有监督的学习里面

65
00:02:30,136 --> 00:02:31,928
我们会输入一个已经label完

66
00:02:31,928 --> 00:02:33,720
就我们已经做好标签的数据

67
00:02:33,720 --> 00:02:35,768
然后呢, 输给我们的输出

68
00:02:35,768 --> 00:02:38,904
中间通过一个损失函数去反馈一个误差

69
00:02:38,904 --> 00:02:39,809
通过这种方式呢

70
00:02:39,809 --> 00:02:42,859
就用深度学习去解决我们有监督的问题

71
00:02:42,859 --> 00:02:44,109
那无监督的问题

72
00:02:44,109 --> 00:02:46,255
就是无监督学习这一个内容

73
00:02:46,255 --> 00:02:48,700
也是通过一个神经网络去处理的

74
00:02:49,404 --> 00:02:50,300
像强化学习呢

75
00:02:50,300 --> 00:02:52,556
现在也跟深度学习结合的非常紧密

76
00:02:52,556 --> 00:02:53,975
叫做深度强化学习

77
00:02:53,975 --> 00:02:56,407
我之前出过一本书, 也是讲相关的内容

78
00:02:56,407 --> 00:02:58,199
这一面呢, 假设这是一个agent

79
00:02:58,199 --> 00:02:59,543
或者是一个神经网络呢

80
00:02:59,543 --> 00:03:00,951
就会输出一些policy,或者value

81
00:03:00,951 --> 00:03:02,999
给最后的输出, 给出一个action

82
00:03:02,999 --> 00:03:05,792
然后在环境当中, 给出一个新的状态

83
00:03:05,792 --> 00:03:08,750
要不断地去进行一个学习反馈的过程

84
00:03:08,750 --> 00:03:12,270
这个就是深度学习跟整个AI的三大主流方式

85
00:03:12,270 --> 00:03:14,270
的一种结合的形态

86
00:03:15,167 --> 00:03:17,917
那既然我们了解完整个AI的过程

87
00:03:17,917 --> 00:03:20,797
我们现在来看看深度学习的网络模型的

88
00:03:20,797 --> 00:03:24,489
一些结构, 还有设计, 还有它的演进的方式

89
00:03:25,275 --> 00:03:26,939
下面这个图很神奇的

90
00:03:26,939 --> 00:03:28,486
就是一层一层套下来

91
00:03:28,486 --> 00:03:31,302
它实际上是一个卷积神经网络

92
00:03:31,750 --> 00:03:35,014
通过反卷积和内向可视化的工程当中

93
00:03:35,014 --> 00:03:35,730
我们可以发现

94
00:03:35,730 --> 00:03:37,394
其实每一层神经网络

95
00:03:37,394 --> 00:03:39,186
它都有不同的感知

96
00:03:39,186 --> 00:03:41,362
或者不同的特征的提取层

97
00:03:41,362 --> 00:03:43,362
不同的特征或者不同的层数

98
00:03:43,362 --> 00:03:45,778
会提取不同的特征出来

99
00:03:45,778 --> 00:03:46,610
直到最后呢

100
00:03:46,610 --> 00:03:49,993
我们可以看到我们想得到的哪些具体的信号

101
00:03:50,603 --> 00:03:51,179
接下来呢

102
00:03:51,179 --> 00:03:52,331
我们主要是看一看

103
00:03:52,331 --> 00:03:55,083
神经网络的一个主要的计算的范式

104
00:03:55,083 --> 00:03:56,350
在神经网络里面呢

105
00:03:56,350 --> 00:03:57,630
它的主要的计算模式

106
00:03:57,630 --> 00:03:59,701
大部分都是乘加

107
00:03:59,701 --> 00:04:01,339
乘加这种模式

108
00:04:01,339 --> 00:04:02,697
或者乘加这种计算

109
00:04:02,697 --> 00:04:06,629
已经占了整个神经网络超过90%的计算量

110
00:04:06,949 --> 00:04:10,500
而乘加可以化为一个权重求和的过程

111
00:04:10,500 --> 00:04:12,676
像下面左下角的这个图

112
00:04:12,676 --> 00:04:15,236
就是一个经典的神经网络

113
00:04:15,750 --> 00:04:16,774
圈圈里面

114
00:04:16,774 --> 00:04:17,926
左边的这个

115
00:04:17,926 --> 00:04:19,462
 就是我们的求和

116
00:04:19,462 --> 00:04:21,638
右边的这个就是我们的激活

117
00:04:21,638 --> 00:04:24,761
激活层跟求和层就是一个简单的神经元的组合

118
00:04:24,761 --> 00:04:27,641
每个圈圈代表一个简单的神经元

119
00:04:27,641 --> 00:04:29,726
它会做一个乘加的操作

120
00:04:29,726 --> 00:04:31,726
或者说矩阵相乘的操作

121
00:04:32,125 --> 00:04:34,877
右边这个就是我们的激活函数

122
00:04:34,877 --> 00:04:38,484
也是里面的圆圈里面右边的一个F

123
00:04:38,484 --> 00:04:38,934
这里面呢

124
00:04:38,934 --> 00:04:41,958
我们主要说明的就是神经网络的主要计算

125
00:04:41,958 --> 00:04:43,366
就是权重的求和

126
00:04:43,366 --> 00:04:45,366
乘加的操作

127
00:04:46,250 --> 00:04:47,146
那下面呢

128
00:04:47,146 --> 00:04:48,874
有了这个知识面以后呢

129
00:04:48,874 --> 00:04:52,394
我们看看主流的神经网络或者网络模型的架构

130
00:04:52,394 --> 00:04:53,482
主要有四种

131
00:04:53,928 --> 00:04:56,775
第一种，就是全连接网络

132
00:04:56,775 --> 00:04:58,775
Fully Connected Layer

133
00:04:58,775 --> 00:05:01,008
第二种就是卷积层

134
00:05:01,008 --> 00:05:02,271
Convolution Layer

135
00:05:02,271 --> 00:05:02,783
这两层呢

136
00:05:02,783 --> 00:05:05,321
我相信大家也是非常之熟悉的

137
00:05:05,321 --> 00:05:08,009
像全连接网络里面最出名的就是MLP

138
00:05:08,009 --> 00:05:10,505
在卷积层最出名的就是CNN

139
00:05:10,505 --> 00:05:12,505
这种网络模型架构

140
00:05:12,505 --> 00:05:14,244
下面我们看看另外两种

141
00:05:14,244 --> 00:05:15,875
也是非常非常的火的

142
00:05:15,875 --> 00:05:17,219
像循环神经网络呢

143
00:05:17,219 --> 00:05:18,435
其实在早些年

144
00:05:18,435 --> 00:05:21,586
主要是用在信号的处理和自然语言的处理

145
00:05:21,586 --> 00:05:22,581
不过后来呢

146
00:05:22,581 --> 00:05:23,795
出现了注意力机制

147
00:05:23,795 --> 00:05:25,011
就是我们的Attention

148
00:05:25,011 --> 00:05:26,163
或者Transformer的结构

149
00:05:26,163 --> 00:05:30,009
引爆了整个或者改变了整个序列数据的处理

150
00:05:30,009 --> 00:05:32,057
下面介绍的这四种呢

151
00:05:32,057 --> 00:05:34,651
就是现阶段的主要的网络模型结构

152
00:05:34,651 --> 00:05:35,995
大家了解一下

153
00:05:35,995 --> 00:05:37,469
知道一下这些名词就好了

154
00:05:37,469 --> 00:05:39,971
如果你没有了解过太多的深度学习

155
00:05:39,971 --> 00:05:43,363
大家可以根据相关的词频或者相关的知识

156
00:05:43,363 --> 00:05:44,963
去反向的检索

157
00:05:46,000 --> 00:05:46,896
接下来

158
00:05:46,896 --> 00:05:48,141
我们以卷积的计算

159
00:05:48,141 --> 00:05:50,736
或者卷积神经网络这个CNN层呢

160
00:05:50,736 --> 00:05:51,809
作为一个主要的例子

161
00:05:51,809 --> 00:05:52,386
我们看看

162
00:05:52,386 --> 00:05:53,750
里面有哪些不一样的东西

163
00:05:53,750 --> 00:05:54,723
那首先呢

164
00:05:54,723 --> 00:05:56,387
还是刚才的那一段话

165
00:05:56,387 --> 00:05:57,772
我们的key operation

166
00:05:57,772 --> 00:05:59,091
就是主要的计算模式呢

167
00:05:59,091 --> 00:06:01,785
集中在我们的乘加的操作

168
00:06:01,785 --> 00:06:03,945
占了大量的一个计算

169
00:06:03,945 --> 00:06:05,482
所以说神经网络模型啊

170
00:06:05,482 --> 00:06:07,722
大部分都是通过矩阵的相乘

171
00:06:07,722 --> 00:06:09,225
矩阵的操作

172
00:06:09,225 --> 00:06:10,660
进行一个处理的

173
00:06:10,660 --> 00:06:13,422
那下面我们看看卷积计算里面呢

174
00:06:13,422 --> 00:06:14,689
有个很重要的特点

175
00:06:14,689 --> 00:06:16,929
就是会有大量的channel

176
00:06:16,929 --> 00:06:18,929
还有大量的output channel

177
00:06:18,929 --> 00:06:20,929
这个channel是非常的多的

178
00:06:20,929 --> 00:06:22,209
不管是权重也好

179
00:06:22,209 --> 00:06:23,169
feature map也好

180
00:06:23,169 --> 00:06:24,520
包括我们的输出也好

181
00:06:24,520 --> 00:06:26,248
都有大量的channel

182
00:06:26,248 --> 00:06:28,504
channel非常非常的深

183
00:06:28,504 --> 00:06:30,616
另外一方面，除了channel之外

184
00:06:30,616 --> 00:06:32,792
我们还有非常多的batch size

185
00:06:32,792 --> 00:06:35,502
就是我们的feature map有非常多轮

186
00:06:35,502 --> 00:06:37,208
所以它会有NCHW

187
00:06:37,208 --> 00:06:40,142
里面的N就变得非常的大了

188
00:06:40,142 --> 00:06:42,767
另外呢，也在一些特殊的情况下

189
00:06:42,767 --> 00:06:44,767
例如遥感的神经网络模型

190
00:06:44,767 --> 00:06:48,009
它的一个图片的输入是非常非常的庞大的

191
00:06:48,009 --> 00:06:50,137
可能万乘以万的级别

192
00:06:50,137 --> 00:06:52,505
就证明我们整个feature map膨胀

193
00:06:53,559 --> 00:06:55,867
最后我们再看一个dynamic shape

194
00:06:55,867 --> 00:06:57,083
就是我们的shape啊

195
00:06:57,083 --> 00:06:59,899
在不同的层里面会不断地去变化

196
00:06:59,899 --> 00:07:01,834
或者在每一层里面

197
00:07:01,834 --> 00:07:03,434
它的shape呢是不一样的

198
00:07:03,434 --> 00:07:04,074
有些层呢

199
00:07:04,074 --> 00:07:06,018
它的长宽高

200
00:07:06,018 --> 00:07:08,642
跟下一层的长宽高也是不相同的

201
00:07:08,642 --> 00:07:12,293
所以整体我们叫做varies across layer

202
00:07:12,293 --> 00:07:13,620
这种方式

203
00:07:14,120 --> 00:07:16,649
以卷积神经网络讲完例子之后呢

204
00:07:16,649 --> 00:07:19,217
我们现在来看看一些非常经典的

205
00:07:19,217 --> 00:07:21,217
神经网络的模型

206
00:07:21,217 --> 00:07:22,322
那我们这里面呢

207
00:07:22,322 --> 00:07:26,546
从1998年开始到2019年

208
00:07:26,546 --> 00:07:28,495
也确实还挺近的

209
00:07:28,495 --> 00:07:30,925
跨度接近20年的时间

210
00:07:30,925 --> 00:07:33,485
我们的网络模型从LeNet-5

211
00:07:33,485 --> 00:07:36,726
到我们后来的谷歌出的EfficientNet

212
00:07:36,726 --> 00:07:38,726
网络模型的层数

213
00:07:38,726 --> 00:07:40,054
网络模型的计算量

214
00:07:40,054 --> 00:07:41,654
网络模型的权重

215
00:07:41,654 --> 00:07:43,284
也是越来越大

216
00:07:43,284 --> 00:07:44,395
越来越夸张

217
00:07:44,395 --> 00:07:46,932
占用我们的内存也是越来越多的

218
00:07:48,124 --> 00:07:51,899
横轴坐标就是计算的一个FLOPs

219
00:07:51,899 --> 00:07:54,750
纵坐标就是top5的一个accuracy

220
00:07:54,750 --> 00:07:56,798
在ImageNet这个数据集里面

221
00:07:56,798 --> 00:07:59,122
那可以看到网络模型越大

222
00:07:59,122 --> 00:08:00,914
那下面这几个我们先忽略

223
00:08:00,914 --> 00:08:03,053
网络模型越大

224
00:08:03,053 --> 00:08:05,485
它需要的算力也就越高

225
00:08:05,485 --> 00:08:10,418
但是同时它的网络模型的精度也就越好

226
00:08:10,418 --> 00:08:11,506
所以我们可以看到

227
00:08:11,506 --> 00:08:13,874
网络模型大步进消耗大量的算力

228
00:08:13,874 --> 00:08:17,010
它也确实对我们的整个精度是有提升的

229
00:08:17,010 --> 00:08:18,930
那基于上面这几点呢

230
00:08:18,930 --> 00:08:21,490
我们对整个AI的计算模式

231
00:08:21,490 --> 00:08:23,070
提出了几个思考

232
00:08:23,070 --> 00:08:25,434
这也是我所总结的一些思考

233
00:08:25,434 --> 00:08:26,855
首先我们芯片

234
00:08:26,855 --> 00:08:28,855
或者我们的AI计算的模式里面呢

235
00:08:28,855 --> 00:08:32,615
我们需要支持神经网络模型的一个计算的逻辑

236
00:08:32,615 --> 00:08:34,471
计算逻辑非常的重要

237
00:08:34,471 --> 00:08:37,840
利用我们刚才讲到的权重的数据的共享

238
00:08:37,840 --> 00:08:40,113
以便于我们对神经网络神经元呢

239
00:08:40,113 --> 00:08:41,585
进行一个矩阵的求和

240
00:08:41,585 --> 00:08:44,205
那除了卷积全连接这种呢

241
00:08:44,205 --> 00:08:45,999
我们同时也是需要支持

242
00:08:45,999 --> 00:08:47,843
像激活softmax等

243
00:08:47,843 --> 00:08:49,583
非常多的vector的计算

244
00:08:49,583 --> 00:08:52,174
所以我们不仅需要(quip?)等计算

245
00:08:52,174 --> 00:08:54,550
我们还需要大量的vector scalar的计算,

246
00:08:54,550 --> 00:08:55,318
那第二种呢

247
00:08:55,318 --> 00:08:58,006
就是需要支持高维的张量的存储

248
00:08:58,006 --> 00:08:58,831
可以看到

249
00:08:58,831 --> 00:09:01,343
其实我们刚才讲到了在神经网络里面

250
00:09:01,343 --> 00:09:03,023
有NCHW这种数据结构

251
00:09:03,023 --> 00:09:04,223
就张量的数据结构

252
00:09:04,223 --> 00:09:05,823
对于张量的数据结构呢

253
00:09:05,823 --> 00:09:07,823
我们的内存地址确实

254
00:09:07,823 --> 00:09:09,919
如果能够支持随机或者自动的索引

255
00:09:09,919 --> 00:09:13,482
能够大大的去增加我们计算的效率

256
00:09:13,482 --> 00:09:15,594
或者内存读取的效率

257
00:09:16,306 --> 00:09:18,063
另外呢在神经网络里面呢

258
00:09:18,063 --> 00:09:19,943
会有大的Channel或者大的feature map

259
00:09:19,943 --> 00:09:20,903
那这个时候呢

260
00:09:20,903 --> 00:09:23,922
如何对我们的张量进行高效的加载

261
00:09:23,922 --> 00:09:24,547
(也是非常重要的)

262
00:09:24,547 --> 00:09:25,443
接着第三种

263
00:09:25,443 --> 00:09:28,899
就是支持常用的神经网络的网络模型结构

264
00:09:28,899 --> 00:09:30,002
我们刚才讲到的

265
00:09:30,002 --> 00:09:32,162
像卷积, MatMul, Transformer, LSTM

266
00:09:32,162 --> 00:09:35,938
等高效的一些常用的或者经典的算法

267
00:09:35,938 --> 00:09:39,074
确实你必须要支持得越多越好

268
00:09:39,374 --> 00:09:40,374
最后一点就是

269
00:09:40,374 --> 00:09:43,234
需要快速的应对新的AI算法和结构

270
00:09:43,234 --> 00:09:44,658
而像现在GPU

271
00:09:44,658 --> 00:09:47,223
为什么大家用得越来越多在AI领域

272
00:09:47,223 --> 00:09:50,807
是因为它确实能够应对新的快速的AI算法

273
00:09:50,807 --> 00:09:52,535
而像TPU或者NPU

274
00:09:52,535 --> 00:09:54,112
诶！不能提不能提

275
00:09:54,112 --> 00:09:55,648
大家懂的都懂

276
00:10:01,764 --> 00:10:03,748
在接下来第二大内容里面呢

277
00:10:03,748 --> 00:10:06,960
我们看看模型量化和网络剪枝

278
00:10:06,960 --> 00:10:08,240
对整个计算模式的

279
00:10:08,240 --> 00:10:10,750
一些改变或者冲击还有思考

280
00:10:11,393 --> 00:10:13,179
下面有两张图

281
00:10:13,179 --> 00:10:14,139
上面一张图呢

282
00:10:14,139 --> 00:10:17,467
就是模型的剪枝或者网络的剪枝

283
00:10:17,467 --> 00:10:18,619
可以看到网络的剪枝呢

284
00:10:18,619 --> 00:10:19,451
在一个权重里面

285
00:10:19,451 --> 00:10:24,443
我们会剪掉或者去掉很多没有用的一些数据

286
00:10:24,443 --> 00:10:24,955
但是呢

287
00:10:24,955 --> 00:10:26,619
我们每一个数据的元素呢

288
00:10:26,619 --> 00:10:29,115
都是保持在32bit里面

289
00:10:29,115 --> 00:10:30,132
32比特里面

290
00:10:30,132 --> 00:10:31,092
继承不会变

291
00:10:31,092 --> 00:10:33,637
而量化就是我们所有的权重了

292
00:10:33,637 --> 00:10:34,812
其实都在的

293
00:10:34,812 --> 00:10:36,527
但是每一个数据

294
00:10:36,527 --> 00:10:39,359
逻辑已经做了一个低比特量化了

295
00:10:39,359 --> 00:10:42,584
原来的32比特变到现在的8比特

296
00:10:43,375 --> 00:10:47,421
有了对量化压缩和模型剪枝两个基础概念

297
00:10:47,421 --> 00:10:48,175
之后我们来看看

298
00:10:48,175 --> 00:10:50,033
低比特量化主要的特征

299
00:10:50,033 --> 00:10:50,984
那这个图呢

300
00:10:50,984 --> 00:10:53,666
红色的或者红色阐释的这一个呢

301
00:10:53,666 --> 00:10:56,226
就是经过量化后剩下的数据

302
00:10:56,226 --> 00:10:58,226
而原来的蓝色

303
00:10:58,226 --> 00:11:00,226
底下蓝色的就是原始的数据

304
00:11:00,268 --> 00:11:02,334
下面我们来看一下

305
00:11:02,334 --> 00:11:05,158
像量化这几年非常热门的研究

306
00:11:05,158 --> 00:11:06,822
我总结了四个

307
00:11:06,822 --> 00:11:08,822
第一个就是感知量化训练

308
00:11:08,822 --> 00:11:11,318
像8比特或更低比特的一些量化

309
00:11:11,318 --> 00:11:13,693
确实需要跟训练相结合的

310
00:11:14,365 --> 00:11:17,625
第二种就是减少计算的比特位

311
00:11:17,625 --> 00:11:20,761
那像以前呢有一些8比特的量化

312
00:11:20,761 --> 00:11:23,407
后来又有一些二值化的网络模型的出现

313
00:11:23,407 --> 00:11:26,281
为的就是减少更多的比特

314
00:11:26,281 --> 00:11:28,009
那还有非线性的量化

315
00:11:28,009 --> 00:11:29,333
像Log-Net这种呢

316
00:11:29,333 --> 00:11:32,789
把原来的我们的量化从线性的变成非线性的

317
00:11:32,789 --> 00:11:36,469
最后还有一些减少权重的或者激活的计算的

318
00:11:36,469 --> 00:11:39,528
例如华为诺亚研究室提出的一种加法网络

319
00:11:39,528 --> 00:11:43,438
或者业界有一些很好玩的异或非的网络模型

320
00:11:43,438 --> 00:11:46,958
这四种都是网络模型量化相关的热门研究

321
00:11:47,520 --> 00:11:49,646
下面呢我们来看一下剪枝

322
00:11:49,646 --> 00:11:53,950
假设左边这个图就是一个简单的多层的神经网络

323
00:11:54,450 --> 00:11:57,458
像第一种就是对我们的全连接进行剪枝

324
00:11:57,458 --> 00:11:59,517
把我们的连接的线剪掉

325
00:11:59,749 --> 00:12:02,373
第二种就是对我们的神经元进行剪枝

326
00:12:02,373 --> 00:12:05,701
就把我们的神经元绿色的这个圈圈把它剪掉

327
00:12:05,701 --> 00:12:07,141
所以有两种的方式

328
00:12:07,440 --> 00:12:10,225
往左边走就是非结构化的剪枝

329
00:12:10,225 --> 00:12:12,321
往右走就是结构化的剪枝

330
00:12:12,321 --> 00:12:14,217
所谓的非结构化的剪辑就是

331
00:12:14,217 --> 00:12:14,985
我随机地

332
00:12:14,985 --> 00:12:16,645
随机对我们的权重

333
00:12:16,645 --> 00:12:17,733
或者我们的神经元

334
00:12:17,733 --> 00:12:19,733
像刚才上一页去介绍的

335
00:12:19,733 --> 00:12:21,493
进行随机的剪枝

336
00:12:21,980 --> 00:12:26,281
结构化的剪枝就是我有组织有效率地去剪枝

337
00:12:26,281 --> 00:12:27,357
例如对我们的filter

338
00:12:27,357 --> 00:12:28,324
对我们的channel

339
00:12:28,324 --> 00:12:29,499
对我们的layer

340
00:12:29,499 --> 00:12:31,499
进行分步的剪枝

341
00:12:31,499 --> 00:12:33,164
这种就是叫做结构化

342
00:12:33,164 --> 00:12:35,460
非结构化就更多的随机性

343
00:12:36,176 --> 00:12:39,004
对于模型压缩我们来到最后一个话题

344
00:12:39,004 --> 00:12:41,372
就是AI的一个计算模式的思考

345
00:12:41,372 --> 00:12:44,603
在网络模型压缩里面对我们的第一个思考

346
00:12:44,603 --> 00:12:47,506
就是需要提供不同的低比特的位数

347
00:12:47,506 --> 00:12:48,992
那至于低比特的位数

348
00:12:48,992 --> 00:12:50,912
有可能会提供int8或者int4

349
00:12:50,912 --> 00:12:52,437
（甚至）更低比特的

350
00:12:52,437 --> 00:12:55,050
另外可能我们在M-bits和E-bits里面

351
00:12:55,050 --> 00:12:56,711
我们需要进行一个权衡的

352
00:12:56,711 --> 00:12:59,895
例如有TF32还有BF16

353
00:12:59,895 --> 00:13:03,455
或者引入新的浮点精度去加速我们整体的运算

354
00:13:03,455 --> 00:13:06,229
第二点就是希望能够充分利用硬件

355
00:13:06,229 --> 00:13:07,705
去提升稀疏的计算

356
00:13:07,705 --> 00:13:08,985
实际上我们的神经网络

357
00:13:08,985 --> 00:13:09,926
我们的大模型里面

358
00:13:09,926 --> 00:13:11,078
或者我们的一些小模型

359
00:13:11,078 --> 00:13:13,574
里面有非常多的零值

360
00:13:13,574 --> 00:13:15,942
那这个时候我重复计算是没有意义的

361
00:13:15,942 --> 00:13:19,542
我们能不能利用硬件去提供稀疏化的计算

362
00:13:19,542 --> 00:13:23,662
那这个对我们的GNA网络里面确实非常的奏效

363
00:13:23,662 --> 00:13:26,110
第二种就是希望能够对我们的网络模型

364
00:13:26,110 --> 00:13:27,720
我们剪枝完之后

365
00:13:27,720 --> 00:13:30,595
确实能够减少很多内存的需求

366
00:13:30,595 --> 00:13:32,579
减少我们很多内存IO的通讯

367
00:13:32,579 --> 00:13:35,651
那这个时候稀疏化的网络就变得更加重要了

368
00:13:35,651 --> 00:13:37,163
我们的硬件怎么更好地

369
00:13:37,163 --> 00:13:38,915
去支持我们的稀疏化的网络模型

370
00:13:38,915 --> 00:13:43,147
需要也是我们的整个AI模式进行思考了

371
00:13:44,805 --> 00:13:46,456
好了今天的内容就这么多

372
00:13:46,456 --> 00:13:49,464
我们在对于深度学习或者AI计算模式的思考

373
00:13:49,464 --> 00:13:52,281
主要去讲了经典的网络模型结构

374
00:13:52,281 --> 00:13:53,881
CNN LSTM Transformer

375
00:13:53,881 --> 00:13:55,601
这种经典的网络模型结构

376
00:13:55,601 --> 00:13:57,521
对整个计算模式的一些思考

377
00:13:57,521 --> 00:14:00,881
对我们硬件应该往哪些方面去设计和牵引

378
00:14:00,881 --> 00:14:01,745
做了一个思考

379
00:14:01,745 --> 00:14:04,561
接着我们又去看了一下网络模型量化和剪枝

380
00:14:04,561 --> 00:14:06,201
就是模型的压缩

381
00:14:06,201 --> 00:14:07,761
对整个计算模式

382
00:14:07,761 --> 00:14:11,409
对整个AI芯片带来哪些新的思考的点

383
00:14:12,392 --> 00:14:13,160
卷的不行了

384
00:14:13,160 --> 00:14:13,857
卷的不行了

385
00:14:13,857 --> 00:14:15,697
记得一键三连加关注哦

386
00:14:15,880 --> 00:14:19,440
所有的内容都会开源在下面这条链接里面

387
00:14:19,440 --> 00:14:20,240
摆了个掰

